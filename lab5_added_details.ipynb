{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "qZMPf_KP2Eb0",
      "metadata": {
        "id": "qZMPf_KP2Eb0"
      },
      "source": [
        "# LAB 5\n",
        "\n",
        "* Make a rag or bq database and embed the documents\n",
        "* Make api tool calling for weather and alaska city location\n",
        "---\n",
        "* Test and verify you can send a request and get a weather report or faq on the alaskan department of snow.\n",
        "* Evaluate groundedness?\n",
        "---\n",
        "* Prompt filtering and response validation implemented into the solution (Keep to security guidlines) Kinda tied in to above testing\n",
        "---\n",
        "* Log conversations... Somewhere...\n",
        "---\n",
        "* Deploy onto web app\n",
        "(utalize ai and quick assist to help with front end)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "YEQnHtvGLgZI",
      "metadata": {
        "id": "YEQnHtvGLgZI"
      },
      "source": [
        "## SET UP\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "-w9wk2F0EYNR",
      "metadata": {
        "id": "-w9wk2F0EYNR"
      },
      "outputs": [],
      "source": [
        "! pip install --upgrade --quiet google-cloud-aiplatform google-cloud-aiplatform[evaluation]\n",
        "! pip install --quiet ipytest\n",
        "! pip install --upgrade google-genai\n",
        "! pip install google-cloud-modelarmor\n",
        "! pip install --upgrade google-cloud-bigquery\n",
        "!pip install requests==2.31.0"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "OXrgwLIu3Zji",
      "metadata": {
        "id": "OXrgwLIu3Zji"
      },
      "source": [
        "\n",
        "Copy over data\n",
        "```\n",
        "gsutil cp -r gs://labs.roitraining.com/alaska-dept-of-snow ~/\n",
        "```\n",
        "\n",
        "Create bq data files\n",
        "```\n",
        "export mydataset=\"alaska_dept_faq_db\"\n",
        "export mytable=\"faqs_table\"\n",
        "```\n",
        "```\n",
        "bq mk $mydataset\n",
        "bq load \\\n",
        "  --source_format=CSV \\\n",
        "  --skip_leading_rows=0 \\\n",
        "  $mydataset.$mytable \\\n",
        "  ~/alaska-dept-of-snow/alaska-dept-of-snow-faqs.csv \\\n",
        "  question:STRING,answer:STRING\n",
        "\n",
        "bq ls $mydataset\n",
        "\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "qJnXlw9aLpHB",
      "metadata": {
        "id": "qJnXlw9aLpHB"
      },
      "source": [
        "## Restart Session\n",
        "\n",
        "You have set up env and datasets, you may need to restart before procededing with the rest of the code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "h_Hb9QAuFTTU",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h_Hb9QAuFTTU",
        "outputId": "b0d62c1a-c817-42be-d2d2-64f0b45426e6"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/google/cloud/aiplatform/models.py:52: FutureWarning: Support for google-cloud-storage < 3.0.0 will be removed in a future version of google-cloud-aiplatform. Please upgrade to google-cloud-storage >= 3.0.0.\n",
            "  from google.cloud.aiplatform.utils import gcs_utils\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "from IPython.display import display, Markdown, HTML\n",
        "\n",
        "import requests\n",
        "\n",
        "import pytest\n",
        "import ipytest\n",
        "ipytest.autoconfig()\n",
        "\n",
        "from vertexai.evaluation import (\n",
        "    MetricPromptTemplateExamples,\n",
        "    EvalTask,\n",
        ")\n",
        "\n",
        "from google import genai\n",
        "from google.genai.types import Tool, ToolCodeExecution, GenerateContentConfig, SafetySetting, Part, FunctionDeclaration\n",
        "from google.cloud import modelarmor_v1\n",
        "from google.api_core.client_options import ClientOptions\n",
        "\n",
        "import datetime\n",
        "from google.cloud import bigquery\n",
        "\n",
        "\n",
        "import google.cloud.logging\n",
        "import logging\n",
        "import time\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2YmyN5gtfGaMLXfZysIx26c5",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2YmyN5gtfGaMLXfZysIx26c5",
        "outputId": "c174338b-34ee-4d6f-d6c1-38e555eb259f",
        "tags": []
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cole-gke-sandbox.alaska_dept_faq_db.sql_connected_model\n",
            "cole-gke-sandbox.alaska_dept_faq_db.faqs_table\n",
            "cole-gke-sandbox.alaska_dept_faq_db.withEmbedding\n"
          ]
        }
      ],
      "source": [
        "project_id = !gcloud config get project\n",
        "project_id = project_id[0]\n",
        "location = \"us-central1\"\n",
        "MY_API_KEY=\"\"\n",
        "\n",
        "model = \"gemini-2.5-flash\"\n",
        "\n",
        "\n",
        "#! export MY_API_KEY="\n",
        "# os.environ['project_id'] = project_id # not working I am going to kill someone. TODO rmv\n",
        "# os.environ['location'] = location\n",
        "\n",
        "\n",
        "mydataset = \"alaska_dept_faq_db\"#!echo $mydataset\n",
        "mytable = f\"\"\"{project_id}.{mydataset}.faqs_table\"\"\" #!echo $project_id.$mydataset.$mytable\n",
        "\n",
        "mymodel = f\"\"\"{project_id}.{mydataset}.sql_connected_model\"\"\"\n",
        "embededTable = f\"\"\"{project_id}.{mydataset}.withEmbedding\"\"\"\n",
        "\n",
        "print(mymodel)\n",
        "print(mytable)\n",
        "print(embededTable)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "bX8W6wpCOWNB",
      "metadata": {
        "id": "bX8W6wpCOWNB"
      },
      "outputs": [],
      "source": [
        "# init genai sdk client\n",
        "genai_client = genai.Client(\n",
        "    vertexai=True,\n",
        "    project=project_id,\n",
        "    location=location\n",
        ")\n",
        "\n",
        "# init big query client\n",
        "bq_client = bigquery.Client(project=project_id)\n",
        "datasets = list(bq_client.list_datasets())\n",
        "\n",
        "\n",
        "# init the Cloud Logging client\n",
        "from google.cloud import logging as cloud_logging\n",
        "log_client = google.cloud.logging.Client()\n",
        "log_client.setup_logging()\n",
        "\n",
        "log_handler = log_client.get_default_handler()\n",
        "\n",
        "cloud_logger = logging.getLogger(\"cloudLogger\")\n",
        "cloud_logger.setLevel(logging.INFO)\n",
        "cloud_logger.addHandler(log_handler)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "rCshh_8SZS-L",
      "metadata": {
        "id": "rCshh_8SZS-L"
      },
      "source": [
        "## MODEL ARMOR\n",
        "in the navigation menu\n",
        "Security > ModelArmor\n",
        "create a template and put the template path in the id below.\n",
        "\n",
        "name == ADS_model_armor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "nM-HIi6dWDm7",
      "metadata": {
        "id": "nM-HIi6dWDm7"
      },
      "outputs": [],
      "source": [
        "armor_id = f\"projects/{project_id}/locations/{location}/templates/ADS_model_armor\"\n",
        "\n",
        "\n",
        "armoredClient = modelarmor_v1.ModelArmorClient(\n",
        "    transport=\"rest\",\n",
        "    client_options=ClientOptions(\n",
        "        api_endpoint=f\"modelarmor.{location}.rep.googleapis.com\"\n",
        "    ),\n",
        ")\n",
        "\n",
        "# ## Model safety settings\n",
        "# from langchain_google_vertexai import HarmBlockThreshold, HarmCategory\n",
        "\n",
        "# # Facinating that we just have butchets of designated banned content\n",
        "# safety_settings = {\n",
        "#     HarmCategory.HARM_CATEGORY_UNSPECIFIED: HarmBlockThreshold.BLOCK_NONE,\n",
        "#     HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: HarmBlockThreshold.BLOCK_NONE,\n",
        "#     HarmCategory.HARM_CATEGORY_HARASSMENT: HarmBlockThreshold.BLOCK_LOW_AND_ABOVE,\n",
        "#     HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: HarmBlockThreshold.BLOCK_MEDIUM_AND_ABOVE,\n",
        "#     HarmCategory.HARM_CATEGORY_HATE_SPEECH: HarmBlockThreshold.BLOCK_ONLY_HIGH,\n",
        "# }\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "wa2PO3ZN6VpB",
      "metadata": {
        "id": "wa2PO3ZN6VpB"
      },
      "source": [
        "## Making Tables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "O-QDDAdgQCWk",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O-QDDAdgQCWk",
        "outputId": "696c49b3-b6cd-43e8-92ca-7866604c93d2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CREATE OR REPLACE MODEL `cole-gke-sandbox.alaska_dept_faq_db.sql_connected_model`\n",
            "REMOTE WITH CONNECTION DEFAULT\n",
            "OPTIONS (ENDPOINT = 'text-embedding-005');\n",
            "\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "QueryJob<project=cole-gke-sandbox, location=US, id=59682f1a-465e-47fd-97d4-3bff39712a1e>"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "## Makes a model that is connected to BigQuerry...\n",
        "makeModel = f\"\"\"CREATE OR REPLACE MODEL `{mymodel}`\n",
        "REMOTE WITH CONNECTION DEFAULT\n",
        "OPTIONS (ENDPOINT = 'text-embedding-005');\"\"\"\n",
        "print(makeModel + \"\\n\")\n",
        "bq_client.query(makeModel)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "lXcXOzDcUENZ",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lXcXOzDcUENZ",
        "outputId": "c22addb9-5401-4e48-9b53-2712e7c432ed"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "CREATE OR REPLACE TABLE `cole-gke-sandbox.alaska_dept_faq_db.withEmbedding` AS\n",
            "  SELECT *\n",
            "  FROM ML.GENERATE_EMBEDDING(\n",
            "  MODEL `cole-gke-sandbox.alaska_dept_faq_db.sql_connected_model`,\n",
            "  (\n",
            "    SELECT question, answer,\tCONCAT(question, answer) AS content\n",
            "    FROM `cole-gke-sandbox.alaska_dept_faq_db.faqs_table`)\n",
            "  );\n",
            "\n",
            "\n",
            "<google.cloud.bigquery.table._EmptyRowIterator object at 0x7ae7ef2a2c00>\n",
            "QueryJob<project=cole-gke-sandbox, location=US, id=49ed32a2-d085-4644-b779-71166da95124>\n"
          ]
        }
      ],
      "source": [
        "## Makes a Table that has been embeded\n",
        "embedding = f\"\"\"\n",
        "CREATE OR REPLACE TABLE `{embededTable}` AS\n",
        "  SELECT *\n",
        "  FROM ML.GENERATE_EMBEDDING(\n",
        "  MODEL `{mymodel}`,\n",
        "  (\n",
        "    SELECT question, answer,\tCONCAT(question, answer) AS content\n",
        "    FROM `{mytable}`)\n",
        "  );\n",
        "\"\"\"\n",
        "query_job = bq_client.query(embedding)\n",
        "\n",
        "print(embedding + \"\\n\")\n",
        "\n",
        "results = query_job.result()\n",
        "print(results)\n",
        "print(query_job)\n",
        "if results == None:\n",
        "  print(\"Big Query Client isn't working\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7EBxoBJ69uK9",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7EBxoBJ69uK9",
        "outputId": "3a943d9b-f8ac-4bd3-bd3b-9b20dd38541a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "     tableId      Type    Labels   Time Partitioning   Clustered Fields  \n",
            " --------------- ------- -------- ------------------- ------------------ \n",
            "  faqs_table      TABLE                                                  \n",
            "  withEmbedding   TABLE                                                  \n"
          ]
        }
      ],
      "source": [
        "!bq ls {mydataset}"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fuMMl4od3qLf",
      "metadata": {
        "id": "fuMMl4od3qLf"
      },
      "source": [
        "## Functions\n",
        "\n",
        "* https://www.weather.gov/documentation/services-web-api\n",
        "* https://console.cloud.google.com/google/maps-apis/home;onboard=true?project=qwiklabs-gcp-03-29c3da30d8ab\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5kkKyKiD0_LU",
      "metadata": {
        "id": "5kkKyKiD0_LU"
      },
      "source": [
        "### Sanatize"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "K5BjRjaQVAh1",
      "metadata": {
        "id": "K5BjRjaQVAh1"
      },
      "outputs": [],
      "source": [
        "def sanatize_text(prompt):\n",
        "  # Returns True (1) if is passes the sanatization test\n",
        "  # Returns False (0) if it fails and needs to be disregaurded.\n",
        "  user_prompt_data = modelarmor_v1.types.DataItem(text=prompt)\n",
        "\n",
        "  ## breaks\n",
        "  request = modelarmor_v1.SanitizeUserPromptRequest(\n",
        "    name= armor_id,\n",
        "    user_prompt_data=user_prompt_data,\n",
        "  )\n",
        "  response = armoredClient.sanitize_user_prompt(request=request)\n",
        "  #print(response)\n",
        "  #print(str(response.sanitization_result.filter_match_state))\n",
        "  if response.sanitization_result.filter_match_state == 1:\n",
        "    return 1\n",
        "  else:\n",
        "    return 0\n",
        "\n",
        "\n",
        "def sanitize_response(response_text):\n",
        "  # If response_text has no value, then return an error message\n",
        "  if not response_text:\n",
        "    return \"An unknown error has occured.\"\n",
        "\n",
        "  model_response_data = modelarmor_v1.DataItem(text = response_text)\n",
        "  request = modelarmor_v1.SanitizeModelResponseRequest(\n",
        "      name= armor_id,\n",
        "      model_response_data=model_response_data,\n",
        "      )\n",
        "\n",
        "  response = genai_client.sanitize_model_response(request=request)\n",
        "\n",
        "  # Return the Sanitized text if sensitive data was found.\n",
        "  # If no sensitive data was found, just return the response text passed to the function.\n",
        "  if str(response.sanitization_result.filter_match_state) == \"FilterMatchState.MATCH_FOUND\":\n",
        "    sanitized_text = response.sanitization_result.filter_results[\"sdp\"].sdp_filter_result.deidentify_result.data.text\n",
        "    return sanitized_text\n",
        "  else:\n",
        "    # There was no invalid data, so just return what was sent in\n",
        "    return response_text"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "p21DsNJl1CVQ",
      "metadata": {
        "id": "p21DsNJl1CVQ"
      },
      "source": [
        "### TOOL Function Decleration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "R00v3Cm5WNeC",
      "metadata": {
        "id": "R00v3Cm5WNeC"
      },
      "outputs": [],
      "source": [
        "rag_query_info = FunctionDeclaration(\n",
        "    name=\"rag_query\",\n",
        "    description=\"VectorSearch's user prompt to faq database and returns related content.\",\n",
        "    parameters={\n",
        "        \"type\": \"object\",\n",
        "        \"properties\": {\n",
        "            \"query\": {\n",
        "                \"type\": \"string\",\n",
        "                \"description\": \"The user's question or prompt to search for in the FAQ database.\"\n",
        "            }\n",
        "        },\n",
        "        \"required\": [\"query\"]\n",
        "    },\n",
        ")\n",
        "\n",
        "get_forecast_info = FunctionDeclaration(\n",
        "    name=\"get_forcast\",\n",
        "    description=\"Given a city name returns a json about upcoming weather patterns\",\n",
        "    parameters={\n",
        "        \"type\": \"object\",\n",
        "        \"properties\": {\n",
        "            \"city\": {\n",
        "                \"type\": \"string\",\n",
        "                \"description\": \"A city in Alaska\"\n",
        "            }\n",
        "        },\n",
        "        \"required\": [\"city\"]\n",
        "    },\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8UpThB_AUp3_",
      "metadata": {
        "id": "8UpThB_AUp3_"
      },
      "source": [
        "### API Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "l0vvH1mV3rz5",
      "metadata": {
        "id": "l0vvH1mV3rz5"
      },
      "outputs": [],
      "source": [
        "# Takes user input and return context from faq doc using rag\n",
        "def rag_query(user_input):\n",
        "\n",
        "  ## Embed user query and find closest neighbors\n",
        "  instructions = (f\"\"\"\n",
        "    SELECT base.answer,\tbase.question\n",
        "    FROM VECTOR_SEARCH(\n",
        "      TABLE `{embededTable}`,\n",
        "      'ml_generate_embedding_result',\n",
        "      (\n",
        "        SELECT ml_generate_embedding_result, content AS query\n",
        "        FROM ML.GENERATE_EMBEDDING(\n",
        "          MODEL `{mymodel}`,\n",
        "            (SELECT '{user_input}' AS content))\n",
        "      ),\n",
        "    top_k => 5,\"\"\" +\n",
        "    \"\"\"options => '{\"fraction_lists_to_search\": 0.01}'\n",
        "    )\n",
        "    \"\"\"\n",
        "  )\n",
        "  #print(instructions)\n",
        "\n",
        "  ## Query BQ\n",
        "  res = bq_client.query(instructions)\n",
        "  if res == None:\n",
        "    print(\":( sorry no luck\")\n",
        "\n",
        "  ## Build context\n",
        "  context = \"\"\n",
        "  for row in res:\n",
        "    context += row['question']+ \"\\n\"+ row['answer'] + (\"-\" * 20)\n",
        "\n",
        "  return context"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "iRX2stxLiZKS",
      "metadata": {
        "id": "iRX2stxLiZKS"
      },
      "outputs": [],
      "source": [
        "# Using geolocation api from gcp and the api key MY_API_KEY\n",
        "# return lat long given a city in alaska\n",
        "# if the city does not exsist return none.\n",
        "def get_latlong(city):\n",
        "  state = \"alaska\"\n",
        "  address = f\"{city}, {state}\"\n",
        "\n",
        "  geocode_url = f\"https://maps.googleapis.com/maps/api/geocode/json?address={address}&key={MY_API_KEY}\"\n",
        "\n",
        "  try:\n",
        "    response = requests.get(geocode_url)\n",
        "    response.raise_for_status() # Raise an HTTPError for bad responses (4xx or 5xx)\n",
        "    data = response.json()\n",
        "\n",
        "    if data['status'] == 'OK' and data['results']:\n",
        "      location = data['results'][0]['geometry']['location']\n",
        "      lat = location['lat']\n",
        "      long = location['lng']\n",
        "      return lat, long\n",
        "    else:\n",
        "      print(f\"Could not find coordinates for {city}, {state}. Status: {data['status']}\")\n",
        "      return None, None\n",
        "  except requests.exceptions.RequestException as e:\n",
        "    print(f\"Error making Geocoding API call: {e}\")\n",
        "    return None, None\n",
        "\n",
        "# helper function\n",
        "# Given lat long coordinates return metadata wfo and x,y coordinate for forcast api call\n",
        "# https://api.weather.gov/points/{lat},{long}\n",
        "def get_points_metadata(lat, long):\n",
        "  url = f\"https://api.weather.gov/points/{lat},{long}\"\n",
        "\n",
        "  try:\n",
        "    response = requests.get(url, headers={'User-Agent': 'Google Colab Weather App (your_email@example.com)'})\n",
        "    response.raise_for_status()  # Raise an HTTPError for bad responses (4xx or 5xx)\n",
        "    data = response.json()\n",
        "\n",
        "    # Extracting the required information\n",
        "    properties = data.get('properties', {})\n",
        "    wfo = properties.get('gridId')\n",
        "    x = properties.get('gridX')\n",
        "    y = properties.get('gridY')\n",
        "\n",
        "    if wfo and x is not None and y is not None:\n",
        "      return wfo, x, y\n",
        "    else:\n",
        "      print(f\"Could not find WFO, gridX, or gridY for {lat},{long}. Response properties: {properties}\")\n",
        "      return None, None, None\n",
        "  except requests.exceptions.RequestException as e:\n",
        "    print(f\"Error making NWS /points API call: {e}\")\n",
        "    return None, None, None\n",
        "  y=\"\"\n",
        "  url = f\"https://api.weather.gov/points/{lat},{long}\"\n",
        "\n",
        "  return wfo, x, y\n",
        "\n",
        "\n",
        "# Given a city return a forcast json\n",
        "# It is a list of items for different times\n",
        "# \"name\" = tonight\n",
        "# \"shortForecast\": \"Slight Chance Light Rain\",\n",
        "# \"detailedForecast\": \"A slight chance of rain. Cloudy, with a high near 34. Northeast wind ...\n",
        "def get_forecast(city):\n",
        "\n",
        "  lat_anchorage, long_anchorage = get_latlong(city)\n",
        "  print (f\"{lat_anchorage} {long_anchorage}\")\n",
        "\n",
        "  wfo,grid_x,grid_y = get_points_metadata(lat=lat_anchorage, long=long_anchorage)\n",
        "\n",
        "  # Construct the forecast URL\n",
        "  forecast_url = f\"https://api.weather.gov/gridpoints/{wfo}/{grid_x},{grid_y}/forecast\"\n",
        "\n",
        "  # Make the API call to get the forecast\n",
        "  try:\n",
        "    # NWS API recommends a User-Agent header\n",
        "    response = requests.get(forecast_url, headers={'User-Agent': 'Google Colab Weather App (your_email@example.com)'})\n",
        "    response.raise_for_status()  # Raise an HTTPError for bad responses (4xx or 5xx)\n",
        "    forecast_data = response.json()\n",
        "    return forecast_data['properties']['periods']\n",
        "  except requests.exceptions.RequestException as e:\n",
        "    print(f\"Error making NWS forecast API call: {e}\")\n",
        "    return None\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "LGNdm1yL3E7v",
      "metadata": {
        "id": "LGNdm1yL3E7v"
      },
      "source": [
        "### Finalize Tool calling and Configurations\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "m8B4AFpP3EC9",
      "metadata": {
        "id": "m8B4AFpP3EC9"
      },
      "outputs": [],
      "source": [
        "\n",
        "rag_tool = Tool(functionDeclarations=[rag_query_info])\n",
        "forecast_tool = Tool(functionDeclarations=[get_forecast_info])\n",
        "\n",
        "sys_msg = \"\"\"\n",
        "* You are the chat bot for Alaska's Department of Snow (ADS).\n",
        "* You are friendly, but breif.\n",
        "* You answer questions related to the department and weather in Alaska, if a\n",
        "client asks'a question about something else remind them that you are \"only the\n",
        "agent for ADS, and can not speak for topics outside of that.\n",
        "* Refrence Rag tool when answering FAQ about the department, there may be\n",
        "information related to the topic that can be mentioned as well.\n",
        "\"\"\"\n",
        "\n",
        "config = GenerateContentConfig(\n",
        "    tools=[rag_tool, forecast_tool],\n",
        "    system_instruction=[sys_msg],\n",
        "    # temperature=0,\n",
        "    # max_output_tokens=1024,\n",
        "    # top_k=10,\n",
        "    # top_p=0.5,\n",
        "    )\n",
        "\n",
        "\n",
        "chat = genai_client.chats.create(\n",
        "   model=model,\n",
        "   config=config\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4YrlfC4HLZZA",
      "metadata": {
        "id": "4YrlfC4HLZZA"
      },
      "source": [
        "## GENERATE response function\n",
        "\n",
        "I don't know why, but sometimes it doesn't run the first time? But I havn't been able to trigger it consistantly"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "id": "D8uj9oXGHOmd",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D8uj9oXGHOmd",
        "outputId": "792ffdc3-a2ee-470f-912b-ab4bea871f7a"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:cloudLogger:User Message: how much training do snowplow drivers receive?\n",
            "INFO:httpx:HTTP Request: POST https://us-central1-aiplatform.googleapis.com/v1beta1/projects/cole-gke-sandbox/locations/us-central1/publishers/google/models/gemini-2.5-flash:generateContent \"HTTP/1.1 200 OK\"\n",
            "INFO:cloudLogger:AI Response: Snowplow drivers at ADS receive specialized training in winter driving techniques, equipment maintenance, and safety protocols for extreme cold environments.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Snowplow drivers at ADS receive specialized training in winter driving techniques, equipment maintenance, and safety protocols for extreme cold environments.\n"
          ]
        }
      ],
      "source": [
        "# function calling isn't automatic. This function checks if the AI determins there needs to be\n",
        "# a tool call made and makes it.\n",
        "def generate(user_prompt) -> str:\n",
        "\n",
        "  # Check if User input is safe\n",
        "  if sanatize_text(user_prompt) == False:\n",
        "    return \"I'm sorry this prompt has flagged the security system, please ask something else.\"\n",
        "  else:\n",
        "    # log user input if safe\n",
        "    cloud_logger.info(f\"User Message: {user_prompt}\")\n",
        "\n",
        "  response = chat.send_message(user_prompt)\n",
        "\n",
        "  # Do we need to make a function\n",
        "  if response.candidates[0].content.parts[0].function_call:\n",
        "    function_name = response.candidates[0].content.parts[0].function_call.name\n",
        "    args = response.candidates[0].content.parts[0].function_call.args\n",
        "\n",
        "    # Which function do we call?\n",
        "    if function_name == \"rag_query\":\n",
        "      context = rag_query(args[\"query\"])\n",
        "    elif function_name == \"get_forcast\":\n",
        "      context = get_forecast(args[\"city\"])\n",
        "    else:\n",
        "      context = \"Unknown function call\"\n",
        "\n",
        "    # Get final prompt with proper context\n",
        "    final_prompt = f\"{user_prompt}\\nContext: {context}\"\n",
        "    response = chat.send_message(final_prompt)\n",
        "\n",
        "    # Sanatize AI response\n",
        "    response = sanitize_response(response)\n",
        "\n",
        "  # Log safe AI response and return\n",
        "  cloud_logger.info(f\"AI Response: {response.text}\")\n",
        "  return response.text\n",
        "\n",
        "\n",
        "ans = generate(\"how much training do snowplow drivers receive?\")\n",
        "print(ans)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "yrRd3e42ssT3",
      "metadata": {
        "id": "yrRd3e42ssT3"
      },
      "source": [
        "## TESTING"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "9HoATQqYsukO",
      "metadata": {
        "id": "9HoATQqYsukO"
      },
      "outputs": [],
      "source": [
        "def test_geo_api():\n",
        "  cities = [\"jenue\", \"Anchorage\", \"Fairbanks\"]\n",
        "\n",
        "  lat, long = get_latlong(cities[0])\n",
        "  assert lat == 58.30049329999999\n",
        "  assert long == -134.4201306\n",
        "\n",
        "  lat, long = get_latlong(cities[1])\n",
        "  assert lat == 61.2175758\n",
        "  assert long == -149.8996785\n",
        "\n",
        "  lat, long = get_latlong(cities[2])\n",
        "  assert lat == 64.8400511\n",
        "  assert long == -147.7199756\n",
        "\n",
        "  for city in cities:\n",
        "    lat, long = get_latlong(city)\n",
        "    print(f\"{city}: {lat}, {long}\")\n",
        "\n",
        "def test_geo_bad():\n",
        "  # 63.588753, -154.4930619 <-- this is the bad location\n",
        "  # because I auto the state to alaska if the city is bad it just defaults here.\n",
        "  # It's just how their api is\n",
        "\n",
        "  cities = [\"Madaline\", 123, \"ladada!\"]\n",
        "  for city in cities:\n",
        "    lat, long = get_latlong(city)\n",
        "    assert lat == 63.588753\n",
        "    assert long == -154.4930619\n",
        "\n",
        "def test_get_points_metadata():\n",
        "  city = \"Anchorage\"\n",
        "  lat, long = get_latlong(city)\n",
        "  w,x,y = get_points_metadata(lat=lat, long=long)\n",
        "\n",
        "  print(f\"{w} {x} {y}\")\n",
        "\n",
        "  lat, long = 61.2175758, -149.8996785\n",
        "  w2,x2,y2 = get_points_metadata(lat=lat, long=long)\n",
        "  assert w == w2\n",
        "  assert x == x2\n",
        "  assert y == y2\n",
        "\n",
        "def test_get_points_metadata_bad():\n",
        "  lat, long = 63.588753, -154.4930619\n",
        "  w,x,y = get_points_metadata(lat=lat, long=long)\n",
        "  assert w == \"AFG\"\n",
        "  print(f\"{w} {x} {y}\")\n",
        "\n",
        "def test_get_forecast():\n",
        "  city_name = \"Anchorage\"\n",
        "  forecast = get_forecast(city_name)\n",
        "  print(forecast[0]['name'])\n",
        "  print(forecast[0]['shortForecast'])\n",
        "  print(forecast[0]['detailedForecast'])\n",
        "  assert forecast is not None\n",
        "\n",
        "## RAG test\n",
        "def test_rag_query():\n",
        "  user_input = \"What are the requirements for a snow machine license?\"\n",
        "  context = rag_query(user_input)\n",
        "  print(context)\n",
        "  assert context is not None\n",
        "  assert len(context) > 0\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "g9Hv-lwf20Up",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g9Hv-lwf20Up",
        "outputId": "c724c8e6-b9ea-49a5-f88a-56a18ead218a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[33m                                                                                       [100%]\u001b[0m\n",
            "\u001b[33m========================================= warnings summary =========================================\u001b[0m\n",
            "../usr/local/lib/python3.12/dist-packages/_pytest/config/__init__.py:1290\n",
            "  /usr/local/lib/python3.12/dist-packages/_pytest/config/__init__.py:1290: PytestAssertRewriteWarning: Module already imported so cannot be rewritten; anyio\n",
            "    self._mark_plugins_for_rewrite(hook, disable_autoload)\n",
            "\n",
            "-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html\n",
            "============================================== PASSES ==============================================\n",
            "\u001b[32m\u001b[1m___________________________________________ test_geo_api ___________________________________________\u001b[0m\n",
            "--------------------------------------- Captured stdout call ---------------------------------------\n",
            "jenue: 58.30049329999999, -134.4201306\n",
            "Anchorage: 61.2175758, -149.8996785\n",
            "Fairbanks: 64.8400511, -147.7199756\n",
            "\u001b[32m\u001b[1m_____________________________________ test_get_points_metadata _____________________________________\u001b[0m\n",
            "--------------------------------------- Captured stdout call ---------------------------------------\n",
            "AER 143 236\n",
            "\u001b[32m\u001b[1m___________________________________ test_get_points_metadata_bad ___________________________________\u001b[0m\n",
            "--------------------------------------- Captured stdout call ---------------------------------------\n",
            "AFG 384 76\n",
            "\u001b[32m\u001b[1m________________________________________ test_get_forecast _________________________________________\u001b[0m\n",
            "--------------------------------------- Captured stdout call ---------------------------------------\n",
            "61.2175758 -149.8996785\n",
            "Today\n",
            "Sunny\n",
            "Sunny, with a high near 5. East wind 0 to 5 mph.\n",
            "\u001b[32m\u001b[1m__________________________________________ test_rag_query __________________________________________\u001b[0m\n",
            "--------------------------------------- Captured stdout call ---------------------------------------\n",
            "How do I become a snowplow driver with ADS?\n",
            "Check job postings for equipment operator or driver positions. Applicants typically need a commercial driver’s license (CDL) and relevant experience.--------------------Does ADS offer financial assistance for snow removal equipment?\n",
            "ADS does not provide direct financial assistance. However, some state grants may be available to local governments for purchasing snow removal equipment.--------------------What training do ADS snowplow drivers receive?\n",
            "Drivers undergo specialized training in winter driving techniques, equipment maintenance, and safety protocols for extreme cold environments.--------------------What is the process for requesting a snow berm removal?\n",
            "A berm is the leftover snow pile created by plows. Contact your local ADS district; they may clear excessively large berms if they pose a safety risk.--------------------What is the mission of the Alaska Department of Snow?\n",
            "Our mission is to ensure safe, efficient travel and infrastructure continuity by coordinating snow removal services across the state’s 650,000 square miles.--------------------\n",
            "\u001b[33m\u001b[32m6 passed\u001b[0m, \u001b[33m\u001b[1m1 warning\u001b[0m\u001b[33m in 6.92s\u001b[0m\u001b[0m\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<ExitCode.OK: 0>"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "ipytest.run('-rP')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6F_yiYuUNfzG",
      "metadata": {
        "id": "6F_yiYuUNfzG"
      },
      "source": [
        "### Evaluation Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "_rhFbxhM_AE8",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 450
        },
        "id": "_rhFbxhM_AE8",
        "outputId": "b8144790-34be-4701-f3bc-bc7a743e8be9"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "        \n",
              "    <link rel=\"stylesheet\" href=\"https://fonts.googleapis.com/icon?family=Material+Icons\">\n",
              "    <style>\n",
              "      .view-vertex-resource,\n",
              "      .view-vertex-resource:hover,\n",
              "      .view-vertex-resource:visited {\n",
              "        position: relative;\n",
              "        display: inline-flex;\n",
              "        flex-direction: row;\n",
              "        height: 32px;\n",
              "        padding: 0 12px;\n",
              "          margin: 4px 18px;\n",
              "        gap: 4px;\n",
              "        border-radius: 4px;\n",
              "\n",
              "        align-items: center;\n",
              "        justify-content: center;\n",
              "        background-color: rgb(255, 255, 255);\n",
              "        color: rgb(51, 103, 214);\n",
              "\n",
              "        font-family: Roboto,\"Helvetica Neue\",sans-serif;\n",
              "        font-size: 13px;\n",
              "        font-weight: 500;\n",
              "        text-transform: uppercase;\n",
              "        text-decoration: none !important;\n",
              "\n",
              "        transition: box-shadow 280ms cubic-bezier(0.4, 0, 0.2, 1) 0s;\n",
              "        box-shadow: 0px 3px 1px -2px rgba(0,0,0,0.2), 0px 2px 2px 0px rgba(0,0,0,0.14), 0px 1px 5px 0px rgba(0,0,0,0.12);\n",
              "      }\n",
              "      .view-vertex-resource:active {\n",
              "        box-shadow: 0px 5px 5px -3px rgba(0,0,0,0.2),0px 8px 10px 1px rgba(0,0,0,0.14),0px 3px 14px 2px rgba(0,0,0,0.12);\n",
              "      }\n",
              "      .view-vertex-resource:active .view-vertex-ripple::before {\n",
              "        position: absolute;\n",
              "        top: 0;\n",
              "        bottom: 0;\n",
              "        left: 0;\n",
              "        right: 0;\n",
              "        border-radius: 4px;\n",
              "        pointer-events: none;\n",
              "\n",
              "        content: '';\n",
              "        background-color: rgb(51, 103, 214);\n",
              "        opacity: 0.12;\n",
              "      }\n",
              "      .view-vertex-icon {\n",
              "        font-size: 18px;\n",
              "      }\n",
              "    </style>\n",
              "  \n",
              "        <a class=\"view-vertex-resource\" id=\"view-vertex-resource-1e5c0cd9-a0f0-44cd-a799-f8582b532ff7\" href=\"#view-view-vertex-resource-1e5c0cd9-a0f0-44cd-a799-f8582b532ff7\">\n",
              "          <span class=\"material-icons view-vertex-icon\">list</span>\n",
              "          <span>Browse pre-built metrics</span>\n",
              "        </a>\n",
              "        \n",
              "        <script>\n",
              "          (function () {\n",
              "            const link = document.getElementById('view-vertex-resource-1e5c0cd9-a0f0-44cd-a799-f8582b532ff7');\n",
              "            link.addEventListener('click', (e) => {\n",
              "              if (window.google?.colab?.openUrl) {\n",
              "                window.google.colab.openUrl('https://cloud.google.com/vertex-ai/generative-ai/docs/models/metrics-templates');\n",
              "              } else {\n",
              "                window.open('https://cloud.google.com/vertex-ai/generative-ai/docs/models/metrics-templates', '_blank');\n",
              "              }\n",
              "              e.stopPropagation();\n",
              "              e.preventDefault();\n",
              "            });\n",
              "          })();\n",
              "        </script>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "['coherence',\n",
              " 'fluency',\n",
              " 'safety',\n",
              " 'groundedness',\n",
              " 'instruction_following',\n",
              " 'verbosity',\n",
              " 'text_quality',\n",
              " 'summarization_quality',\n",
              " 'question_answering_quality',\n",
              " 'multi_turn_chat_quality',\n",
              " 'multi_turn_safety',\n",
              " 'pairwise_coherence',\n",
              " 'pairwise_fluency',\n",
              " 'pairwise_safety',\n",
              " 'pairwise_groundedness',\n",
              " 'pairwise_instruction_following',\n",
              " 'pairwise_verbosity',\n",
              " 'pairwise_text_quality',\n",
              " 'pairwise_summarization_quality',\n",
              " 'pairwise_question_answering_quality',\n",
              " 'pairwise_multi_turn_chat_quality',\n",
              " 'pairwise_multi_turn_safety']"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "prompt_list = [\n",
        "    \"What is your mission?\",\n",
        "    \"What is the weather in fairbanks tonight?\",\n",
        "    \"what does ADS stand for?\",\n",
        "    \"How does the DOT&PF prioritize snow removal on state roads and airports?\",\n",
        "    \"How does the department address environmental concerns related to the use of de-icing chemicals and sand?\",\n",
        "    \"Are there any weather alerts for Stika?\",\n",
        "    \"how to handle avalanche emergancies\",\n",
        "]\n",
        "eval_dataset = pd.DataFrame({\n",
        "    \"prompt\": prompt_list,\n",
        "})\n",
        "\n",
        "#Instantiate an EvalTask by associating your dataset & selected metric\n",
        "eval_task = EvalTask(\n",
        "  dataset=eval_dataset,\n",
        "  metrics=[MetricPromptTemplateExamples.Pointwise.GROUNDEDNESS], # optionally also check quesiton answering\n",
        ")\n",
        "\n",
        "MetricPromptTemplateExamples.list_example_metric_names()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "id": "E5zGQCeSTyG1",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 831
        },
        "id": "E5zGQCeSTyG1",
        "outputId": "3de2e93e-8030-4aef-e9f2-a9550170c2df"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:vertexai.evaluation._evaluation:Generating a total of 7 responses from the custom model function.\n",
            "  0%|          | 0/7 [00:00<?, ?it/s]INFO:cloudLogger:User Message: What is the weather in fairbanks tonight?\n",
            "INFO:cloudLogger:User Message: How does the department address environmental concerns related to the use of de-icing chemicals and sand?\n",
            "INFO:cloudLogger:User Message: How does the DOT&PF prioritize snow removal on state roads and airports?\n",
            "INFO:cloudLogger:User Message: What is your mission?\n",
            "INFO:cloudLogger:User Message: what does ADS stand for?\n",
            "INFO:httpx:HTTP Request: POST https://us-central1-aiplatform.googleapis.com/v1beta1/projects/cole-gke-sandbox/locations/us-central1/publishers/google/models/gemini-2.5-flash:generateContent \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://us-central1-aiplatform.googleapis.com/v1beta1/projects/cole-gke-sandbox/locations/us-central1/publishers/google/models/gemini-2.5-flash:generateContent \"HTTP/1.1 200 OK\"\n",
            "INFO:cloudLogger:AI Response: Our mission is to ensure safe, efficient travel and infrastructure continuity by coordinating snow removal services across Alaska's 650,000 square miles.\n",
            " 14%|█▍        | 1/7 [00:10<01:02, 10.41s/it]INFO:httpx:HTTP Request: POST https://us-central1-aiplatform.googleapis.com/v1beta1/projects/cole-gke-sandbox/locations/us-central1/publishers/google/models/gemini-2.5-flash:generateContent \"HTTP/1.1 200 OK\"\n",
            "INFO:cloudLogger:AI Response: I am only the agent for ADS, and can not speak for topics outside of that. My current knowledge base does not contain information on how the department addresses environmental concerns related to de-icing chemicals and sand.\n",
            "INFO:httpx:HTTP Request: POST https://us-central1-aiplatform.googleapis.com/v1beta1/projects/cole-gke-sandbox/locations/us-central1/publishers/google/models/gemini-2.5-flash:generateContent \"HTTP/1.1 200 OK\"\n",
            "INFO:cloudLogger:AI Response: The Alaska Department of Snow (ADS) works with local municipalities and regional offices to schedule and prioritize plowing routes, focusing first on high-traffic roads, emergency routes, and schools. For airports, ADS works with airport authorities under federal regulations to maintain runways and airport access roads, which are considered high priority.\n",
            "INFO:cloudLogger:User Message: Are there any weather alerts for Stika?\n",
            "INFO:cloudLogger:User Message: how to handle avalanche emergancies\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "64.8400511 -147.7199756\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://us-central1-aiplatform.googleapis.com/v1beta1/projects/cole-gke-sandbox/locations/us-central1/publishers/google/models/gemini-2.5-flash:generateContent \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "57.05321929999999 -135.3345513\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://us-central1-aiplatform.googleapis.com/v1beta1/projects/cole-gke-sandbox/locations/us-central1/publishers/google/models/gemini-2.5-flash:generateContent \"HTTP/1.1 200 OK\"\n",
            "INFO:cloudLogger:AI Response: ADS stands for Alaska's Department of Snow.\n",
            " 57%|█████▋    | 4/7 [00:17<00:11,  3.83s/it]INFO:httpx:HTTP Request: POST https://us-central1-aiplatform.googleapis.com/v1beta1/projects/cole-gke-sandbox/locations/us-central1/publishers/google/models/gemini-2.5-flash:generateContent \"HTTP/1.1 200 OK\"\n",
            "INFO:cloudLogger:AI Response: I am only the agent for ADS, and can not speak for topics outside of that. I do not have information on how to handle avalanche emergencies.\n",
            " 71%|███████▏  | 5/7 [00:19<00:06,  3.40s/it]INFO:httpx:HTTP Request: POST https://us-central1-aiplatform.googleapis.com/v1beta1/projects/cole-gke-sandbox/locations/us-central1/publishers/google/models/gemini-2.5-flash:generateContent \"HTTP/1.1 200 OK\"\n",
            " 86%|████████▌ | 6/7 [00:21<00:03,  3.01s/it]INFO:httpx:HTTP Request: POST https://us-central1-aiplatform.googleapis.com/v1beta1/projects/cole-gke-sandbox/locations/us-central1/publishers/google/models/gemini-2.5-flash:generateContent \"HTTP/1.1 200 OK\"\n",
            "100%|██████████| 7/7 [00:26<00:00,  3.75s/it]\n"
          ]
        },
        {
          "ename": "TypeError",
          "evalue": "bad argument type for built-in operation",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-261051190.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m evalresults = eval_task.evaluate(\n\u001b[0m\u001b[1;32m      2\u001b[0m       \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m )\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# # Print the results\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/vertexai/evaluation/eval_task.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, model, prompt_template, experiment_run_name, response_column_name, baseline_model_response_column_name, evaluation_service_qps, retry_timeout, output_file_name)\u001b[0m\n\u001b[1;32m    477\u001b[0m             )\n\u001b[1;32m    478\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 479\u001b[0;31m             eval_result = _evaluation.evaluate(\n\u001b[0m\u001b[1;32m    480\u001b[0m                 \u001b[0mdataset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    481\u001b[0m                 \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/vertexai/evaluation/_evaluation.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(dataset, metrics, model, prompt_template, metric_column_mapping, evaluation_service_qps, retry_timeout)\u001b[0m\n\u001b[1;32m    956\u001b[0m         \u001b[0m_assemble_prompt_for_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevaluation_run_config\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprompt_template\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    957\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 958\u001b[0;31m     _run_model_inference(\n\u001b[0m\u001b[1;32m    959\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    960\u001b[0m         \u001b[0mevaluation_run_config\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mevaluation_run_config\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/vertexai/evaluation/_evaluation.py\u001b[0m in \u001b[0;36m_run_model_inference\u001b[0;34m(model, evaluation_run_config, response_column_name)\u001b[0m\n\u001b[1;32m    493\u001b[0m                     )\n\u001b[1;32m    494\u001b[0m                 \u001b[0;32melif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 495\u001b[0;31m                     _generate_response_from_custom_model_fn(\n\u001b[0m\u001b[1;32m    496\u001b[0m                         \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevaluation_run_config\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_baseline_model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    497\u001b[0m                     )\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/vertexai/evaluation/_evaluation.py\u001b[0m in \u001b[0;36m_generate_response_from_custom_model_fn\u001b[0;34m(model_fn, evaluation_run_config, is_baseline_model)\u001b[0m\n\u001b[1;32m    449\u001b[0m         \u001b[0m_LOGGER\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Failed to generate response from model function: {e}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    450\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 451\u001b[0;31m     \u001b[0mresponses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtask\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtasks\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    452\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mis_baseline_model\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    453\u001b[0m         evaluation_run_config.dataset = eval_dataset.assign(\n",
            "\u001b[0;32m/usr/lib/python3.12/concurrent/futures/_base.py\u001b[0m in \u001b[0;36mresult\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    447\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mCancelledError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    448\u001b[0m                 \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_state\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mFINISHED\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 449\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    450\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    451\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_condition\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/concurrent/futures/_base.py\u001b[0m in \u001b[0;36m__get_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    399\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    400\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 401\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    402\u001b[0m             \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    403\u001b[0m                 \u001b[0;31m# Break a reference cycle with the exception in self._exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/concurrent/futures/thread.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mBaseException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfuture\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-1145588134.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(user_prompt)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0;31m# Sanatize AI response\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m     \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msanitize_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m   \u001b[0;31m# Log safe AI response and return\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-1917037217.py\u001b[0m in \u001b[0;36msanitize_response\u001b[0;34m(response_text)\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m\"An unknown error has occured.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m   \u001b[0mmodel_response_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodelarmor_v1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataItem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresponse_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m   request = modelarmor_v1.SanitizeModelResponseRequest(\n\u001b[1;32m     27\u001b[0m       \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0marmor_id\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/proto/message.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, mapping, ignore_unknown_fields, **kwargs)\u001b[0m\n\u001b[1;32m    732\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    733\u001b[0m         \u001b[0;31m# Create the internal protocol buffer.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 734\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"_pb\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_meta\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpb\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    735\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    736\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_get_pb_type_from_key\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: bad argument type for built-in operation"
          ]
        }
      ],
      "source": [
        "evalresults = eval_task.evaluate(\n",
        "      model=generate,\n",
        ")\n",
        "\n",
        "# # Print the results\n",
        "# for row in evalresults.summary_metrics:\n",
        "#   print(f\"{row} {evalresults.summary_metrics[row]}\")\n",
        "\n",
        "# print(evalresults.metrics_table)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b_YhKzM7hoa8",
      "metadata": {
        "id": "b_YhKzM7hoa8"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "lab5_fin.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
