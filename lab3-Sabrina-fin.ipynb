{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# TASK 3\n",
        "\n",
        "Classification\n",
        "\n",
        "* Create a Python function that uses Gemini to classify user questions into one of the following\n",
        "categories: Employment, General Information, Emergency Services, or Tax Related\n",
        "\n",
        "* Create a second function that generates social media posts for government announcements like\n",
        "weather emergencies, holidays, school closings, etc.\n",
        "\n",
        "Write unit tests for each function using pytest.\n",
        "\n",
        "Use the Google Evaluation API to evaluate and compare Gemini responses from different\n",
        "prompts\n",
        "\n",
        "Keep track of old dataframe outputs/evals?\n"
      ],
      "metadata": {
        "id": "tSpLXfweQOy2"
      },
      "id": "tSpLXfweQOy2"
    },
    {
      "cell_type": "code",
      "id": "7IJT5usfFD43dy1ZP3Awbydr",
      "metadata": {
        "tags": [],
        "id": "7IJT5usfFD43dy1ZP3Awbydr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "630ae47d-efe8-4114-f6f8-1f0ba67b3581"
      },
      "source": [
        "!pip install --upgrade --quiet google-cloud-aiplatform google-cloud-aiplatform[evaluation]\n",
        "!pip install --quiet ipytest"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.1/46.1 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.1/8.1 MB\u001b[0m \u001b[31m58.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.8/10.8 MB\u001b[0m \u001b[31m82.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m119.9/119.9 kB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m278.1/278.1 kB\u001b[0m \u001b[31m25.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.9/5.9 MB\u001b[0m \u001b[31m92.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m319.9/319.9 kB\u001b[0m \u001b[31m26.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m58.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m788.2/788.2 kB\u001b[0m \u001b[31m52.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colabsqlviz 0.2.7 requires protobuf<7.0.0,>=6.31.1, but you have protobuf 5.29.5 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "project_id = !gcloud config get project\n",
        "project_id = project_id[0]\n",
        "\n",
        "location = \"us-central1\"\n",
        "\n"
      ],
      "metadata": {
        "id": "iPb_YdNELRqq"
      },
      "id": "iPb_YdNELRqq",
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import vertexai\n",
        "from vertexai.generative_models import GenerativeModel, GenerationConfig\n",
        "from vertexai.language_models import TextGenerationModel\n",
        "\n",
        "import pytest\n",
        "import ipytest\n",
        "ipytest.autoconfig()\n",
        "\n",
        "from vertexai.evaluation import (\n",
        "    MetricPromptTemplateExamples,\n",
        "    EvalTask,\n",
        "    PairwiseMetric,\n",
        "    PairwiseMetricPromptTemplate,\n",
        "    PointwiseMetric,\n",
        "    PointwiseMetricPromptTemplate,\n",
        ")\n",
        "\n",
        "\n",
        "import datetime\n",
        "import nest_asyncio\n",
        "import pandas as pd\n",
        "from IPython.display import display, Markdown, HTML\n",
        "\n",
        "pd.set_option('display.max_colwidth', None)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DaUhry9lLWQq",
        "outputId": "ed2334c1-315b-474b-b166-470140b2428c"
      },
      "id": "DaUhry9lLWQq",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/google/cloud/aiplatform/models.py:52: FutureWarning: Support for google-cloud-storage < 3.0.0 will be removed in a future version of google-cloud-aiplatform. Please upgrade to google-cloud-storage >= 3.0.0.\n",
            "  from google.cloud.aiplatform.utils import gcs_utils\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "eval_results_to_compare = []"
      ],
      "metadata": {
        "id": "uclRKGlF0_pb"
      },
      "id": "uclRKGlF0_pb",
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# init models and eval_results\n",
        "\n",
        "vertexai.init(project=project_id, location=location)\n",
        "\n",
        "#Variable changes if you wish to test the effects of different models and temps\n",
        "model = \"gemini-2.5-flash\"\n",
        "temperature_val= 0\n",
        "\n",
        "\n",
        "\n",
        "generation_config = {\n",
        "      \"temperature\": temperature_val,\n",
        "      \"top_p\": 0.4,\n",
        "}\n",
        "\n",
        "model = GenerativeModel(\n",
        "  model,\n",
        "  generation_config=generation_config,\n",
        ")"
      ],
      "metadata": {
        "id": "cS0DaFvCL2iR"
      },
      "id": "cS0DaFvCL2iR",
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# I wound up picking instruction following\n",
        "MetricPromptTemplateExamples.list_example_metric_names()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 450
        },
        "id": "6j2xWAJLL6Br",
        "outputId": "4a1c4e6e-1c35-4b06-bbbd-5e8be9c81f25"
      },
      "id": "6j2xWAJLL6Br",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "        \n",
              "    <link rel=\"stylesheet\" href=\"https://fonts.googleapis.com/icon?family=Material+Icons\">\n",
              "    <style>\n",
              "      .view-vertex-resource,\n",
              "      .view-vertex-resource:hover,\n",
              "      .view-vertex-resource:visited {\n",
              "        position: relative;\n",
              "        display: inline-flex;\n",
              "        flex-direction: row;\n",
              "        height: 32px;\n",
              "        padding: 0 12px;\n",
              "          margin: 4px 18px;\n",
              "        gap: 4px;\n",
              "        border-radius: 4px;\n",
              "\n",
              "        align-items: center;\n",
              "        justify-content: center;\n",
              "        background-color: rgb(255, 255, 255);\n",
              "        color: rgb(51, 103, 214);\n",
              "\n",
              "        font-family: Roboto,\"Helvetica Neue\",sans-serif;\n",
              "        font-size: 13px;\n",
              "        font-weight: 500;\n",
              "        text-transform: uppercase;\n",
              "        text-decoration: none !important;\n",
              "\n",
              "        transition: box-shadow 280ms cubic-bezier(0.4, 0, 0.2, 1) 0s;\n",
              "        box-shadow: 0px 3px 1px -2px rgba(0,0,0,0.2), 0px 2px 2px 0px rgba(0,0,0,0.14), 0px 1px 5px 0px rgba(0,0,0,0.12);\n",
              "      }\n",
              "      .view-vertex-resource:active {\n",
              "        box-shadow: 0px 5px 5px -3px rgba(0,0,0,0.2),0px 8px 10px 1px rgba(0,0,0,0.14),0px 3px 14px 2px rgba(0,0,0,0.12);\n",
              "      }\n",
              "      .view-vertex-resource:active .view-vertex-ripple::before {\n",
              "        position: absolute;\n",
              "        top: 0;\n",
              "        bottom: 0;\n",
              "        left: 0;\n",
              "        right: 0;\n",
              "        border-radius: 4px;\n",
              "        pointer-events: none;\n",
              "\n",
              "        content: '';\n",
              "        background-color: rgb(51, 103, 214);\n",
              "        opacity: 0.12;\n",
              "      }\n",
              "      .view-vertex-icon {\n",
              "        font-size: 18px;\n",
              "      }\n",
              "    </style>\n",
              "  \n",
              "        <a class=\"view-vertex-resource\" id=\"view-vertex-resource-9262af48-ed42-465b-89e8-0035737d73a1\" href=\"#view-view-vertex-resource-9262af48-ed42-465b-89e8-0035737d73a1\">\n",
              "          <span class=\"material-icons view-vertex-icon\">list</span>\n",
              "          <span>Browse pre-built metrics</span>\n",
              "        </a>\n",
              "        \n",
              "        <script>\n",
              "          (function () {\n",
              "            const link = document.getElementById('view-vertex-resource-9262af48-ed42-465b-89e8-0035737d73a1');\n",
              "            link.addEventListener('click', (e) => {\n",
              "              if (window.google?.colab?.openUrl) {\n",
              "                window.google.colab.openUrl('https://cloud.google.com/vertex-ai/generative-ai/docs/models/metrics-templates');\n",
              "              } else {\n",
              "                window.open('https://cloud.google.com/vertex-ai/generative-ai/docs/models/metrics-templates', '_blank');\n",
              "              }\n",
              "              e.stopPropagation();\n",
              "              e.preventDefault();\n",
              "            });\n",
              "          })();\n",
              "        </script>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['coherence',\n",
              " 'fluency',\n",
              " 'safety',\n",
              " 'groundedness',\n",
              " 'instruction_following',\n",
              " 'verbosity',\n",
              " 'text_quality',\n",
              " 'summarization_quality',\n",
              " 'question_answering_quality',\n",
              " 'multi_turn_chat_quality',\n",
              " 'multi_turn_safety',\n",
              " 'pairwise_coherence',\n",
              " 'pairwise_fluency',\n",
              " 'pairwise_safety',\n",
              " 'pairwise_groundedness',\n",
              " 'pairwise_instruction_following',\n",
              " 'pairwise_verbosity',\n",
              " 'pairwise_text_quality',\n",
              " 'pairwise_summarization_quality',\n",
              " 'pairwise_question_answering_quality',\n",
              " 'pairwise_multi_turn_chat_quality',\n",
              " 'pairwise_multi_turn_safety']"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def foo1(prompt):\n",
        "  # uses Gemini to classify user questions into one of the following categories:\n",
        "  # Employment, General Information, Emergency Services, or Tax Related\n",
        "\n",
        "  if prompt == \"\" or type(prompt) != str:\n",
        "    return \"N/A\"\n",
        "\n",
        "  instructions = f\"\"\"\n",
        "  classify the user questions into one of the following categories: Employment, General Information, Emergency Services, Tax Related or UNKNOWN.\n",
        "  Examples --\n",
        "  User: \"I hear you are hiring?\"\n",
        "  Output: \"Employment\"\n",
        "  User: \"I think there's a fire in the valley!\"\n",
        "  Output: \"Emergancy Services\n",
        "  --\n",
        "  User: {prompt}\n",
        "  Output:\n",
        "  \"\"\"\n",
        "\n",
        "  response = model.generate_content(\n",
        "    contents=[instructions],\n",
        "  )\n",
        "  return (response.text)"
      ],
      "metadata": {
        "id": "fWImGiZLRj0R"
      },
      "id": "fWImGiZLRj0R",
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def foo2(prompt):\n",
        "  # generates social media posts for government announcements like weather\n",
        "  # emergencies, holidays, school closings, etc.\n",
        "  if prompt == \"\" or type(prompt) != str:\n",
        "    return None\n",
        "  instructions = \"\"\"\n",
        "  Return a formal and proffesional statement alerting the public based on the user prompt\n",
        "  Weather, Emergancies, Holidays, School Closing, ect.\n",
        "  \"\"\"\n",
        "  response = model.generate_content(\n",
        "    contents=[instructions, prompt]\n",
        "  )\n",
        "  return response.text\n"
      ],
      "metadata": {
        "id": "MtciemCVRnO1"
      },
      "id": "MtciemCVRnO1",
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# previous prompt\n",
        "\"\"\"You are incharge of the PR department for the government's social media posts.\n",
        "                  You make announcements for Weather, Emergancies, Holidays, School Closing, ect.\n",
        "                  Translate the user prompt into a breif formal and proffesional statement\n",
        "                  Only return the translated statement ready to be posted\"\"\"\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "0fxMlhlezAL7"
      },
      "id": "0fxMlhlezAL7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Pytest Basic\n",
        "def test_foo1_bad_input():\n",
        "\n",
        "  prompt = 123\n",
        "  res = foo1(prompt)\n",
        "  assert res == \"N/A\"\n",
        "\n",
        "  prompt = \"\"\n",
        "  res = foo1(prompt)\n",
        "  assert res == \"N/A\"\n",
        "\n",
        "\n",
        "  prompt = \"She sells sea shells by the sea shore\"\n",
        "  res = foo1(prompt)\n",
        "  assert res == \"UNKNOWN\"\n",
        "\n",
        "def test_foo1():\n",
        "  res = foo1(\"Can you tell me a bit about the town?\")\n",
        "  assert res == \"General Information\"\n",
        "\n",
        "  res = foo1(\"Is my W2 form important?\")\n",
        "  assert res == \"Tax Related\"\n",
        "\n",
        "def test_2_bad():\n",
        "  prompt = 123\n",
        "  res = foo2(prompt)\n",
        "  assert res == None\n",
        "\n",
        "  prompt = \"\"\n",
        "  res = foo2(prompt)\n",
        "  assert res == None\n",
        "ipytest.run('-rP')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0ypqSoVpl0ap",
        "outputId": "6aeb5f69-b8fe-4ebb-bab4-4f39094e5578"
      },
      "id": "0ypqSoVpl0ap",
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m                                                                                          [100%]\u001b[0m\n",
            "============================================== PASSES ==============================================\n",
            "\u001b[32m\u001b[32m\u001b[1m3 passed\u001b[0m\u001b[32m in 2.68s\u001b[0m\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<ExitCode.OK: 0>"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\"But what about evaluations based on feels~\" I hear you cry.\n",
        "Fear not, I will cover that next.\n",
        "foo2 is more dependent on tone. So it will be the focus of my evaluations."
      ],
      "metadata": {
        "id": "bm2I4Lk7t2aq"
      },
      "id": "bm2I4Lk7t2aq"
    },
    {
      "cell_type": "code",
      "source": [
        "## SETUP\n",
        "prompts = [\n",
        "    \"Hurricane warnning for florida\",\n",
        "    \"Veterens day comment\",\n",
        "    \"Icy Roads - State of Emergancy\",\n",
        "]\n",
        "\n",
        "eval_dataset = pd.DataFrame({\n",
        "    \"prompt\": prompts,\n",
        "})"
      ],
      "metadata": {
        "id": "fxwZrTKFqFph"
      },
      "id": "fxwZrTKFqFph",
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## TASK\n",
        "#Instantiate an EvalTask by associating your dataset & selected metric\n",
        "eval_task = EvalTask(\n",
        "  dataset=eval_dataset,\n",
        "  metrics=[MetricPromptTemplateExamples.Pointwise.INSTRUCTION_FOLLOWING],\n",
        ")"
      ],
      "metadata": {
        "id": "uNkAcXvivISH"
      },
      "id": "uNkAcXvivISH",
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## TESTING AND COMPARISON HERE\n",
        "you can change the above code and re-run this here\n",
        "If you change the prompt update the EVAL task and data set as well."
      ],
      "metadata": {
        "id": "w0TDx0v71acq"
      },
      "id": "w0TDx0v71acq"
    },
    {
      "cell_type": "code",
      "source": [
        "## Results\n",
        "eval_result = eval_task.evaluate(\n",
        "  model=model\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 200
        },
        "id": "gjZcTMJCvuVU",
        "outputId": "6c571477-5d4e-4b08-a7b7-7e8c8cbf04c2"
      },
      "id": "gjZcTMJCvuVU",
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:vertexai.evaluation._evaluation:Generating a total of 3 responses from Gemini model gemini-2.5-flash.\n",
            "100%|██████████| 3/3 [00:13<00:00,  4.46s/it]\n",
            "INFO:vertexai.evaluation._evaluation:All 3 responses are successfully generated from Gemini model gemini-2.5-flash.\n",
            "INFO:vertexai.evaluation._evaluation:Multithreaded Batch Inference took: 13.377640911998242 seconds.\n",
            "INFO:vertexai.evaluation._evaluation:Computing metrics with a total of 3 Vertex Gen AI Evaluation Service API requests.\n",
            "100%|██████████| 3/3 [00:07<00:00,  2.35s/it]\n",
            "INFO:vertexai.evaluation._evaluation:All 3 metric requests are successfully computed.\n",
            "INFO:vertexai.evaluation._evaluation:Evaluation Took:7.068885298000168 seconds\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "        \n",
              "    <link rel=\"stylesheet\" href=\"https://fonts.googleapis.com/icon?family=Material+Icons\">\n",
              "    <style>\n",
              "      .view-vertex-resource,\n",
              "      .view-vertex-resource:hover,\n",
              "      .view-vertex-resource:visited {\n",
              "        position: relative;\n",
              "        display: inline-flex;\n",
              "        flex-direction: row;\n",
              "        height: 32px;\n",
              "        padding: 0 12px;\n",
              "          margin: 4px 18px;\n",
              "        gap: 4px;\n",
              "        border-radius: 4px;\n",
              "\n",
              "        align-items: center;\n",
              "        justify-content: center;\n",
              "        background-color: rgb(255, 255, 255);\n",
              "        color: rgb(51, 103, 214);\n",
              "\n",
              "        font-family: Roboto,\"Helvetica Neue\",sans-serif;\n",
              "        font-size: 13px;\n",
              "        font-weight: 500;\n",
              "        text-transform: uppercase;\n",
              "        text-decoration: none !important;\n",
              "\n",
              "        transition: box-shadow 280ms cubic-bezier(0.4, 0, 0.2, 1) 0s;\n",
              "        box-shadow: 0px 3px 1px -2px rgba(0,0,0,0.2), 0px 2px 2px 0px rgba(0,0,0,0.14), 0px 1px 5px 0px rgba(0,0,0,0.12);\n",
              "      }\n",
              "      .view-vertex-resource:active {\n",
              "        box-shadow: 0px 5px 5px -3px rgba(0,0,0,0.2),0px 8px 10px 1px rgba(0,0,0,0.14),0px 3px 14px 2px rgba(0,0,0,0.12);\n",
              "      }\n",
              "      .view-vertex-resource:active .view-vertex-ripple::before {\n",
              "        position: absolute;\n",
              "        top: 0;\n",
              "        bottom: 0;\n",
              "        left: 0;\n",
              "        right: 0;\n",
              "        border-radius: 4px;\n",
              "        pointer-events: none;\n",
              "\n",
              "        content: '';\n",
              "        background-color: rgb(51, 103, 214);\n",
              "        opacity: 0.12;\n",
              "      }\n",
              "      .view-vertex-icon {\n",
              "        font-size: 18px;\n",
              "      }\n",
              "    </style>\n",
              "  \n",
              "        <a class=\"view-vertex-resource\" id=\"view-vertex-resource-dc1d57e6-23cc-4bc6-9a6c-5190f3beb61a\" href=\"#view-view-vertex-resource-dc1d57e6-23cc-4bc6-9a6c-5190f3beb61a\">\n",
              "          <span class=\"material-icons view-vertex-icon\">bar_chart</span>\n",
              "          <span>View evaluation results</span>\n",
              "        </a>\n",
              "        \n",
              "        <script>\n",
              "          (function () {\n",
              "            const link = document.getElementById('view-vertex-resource-dc1d57e6-23cc-4bc6-9a6c-5190f3beb61a');\n",
              "            link.addEventListener('click', (e) => {\n",
              "              if (window.google?.colab?.openUrl) {\n",
              "                window.google.colab.openUrl('https://cloud.google.com/vertex-ai/generative-ai/docs/models/view-evaluation');\n",
              "              } else {\n",
              "                window.open('https://cloud.google.com/vertex-ai/generative-ai/docs/models/view-evaluation', '_blank');\n",
              "              }\n",
              "              e.stopPropagation();\n",
              "              e.preventDefault();\n",
              "            });\n",
              "          })();\n",
              "        </script>\n",
              "    "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "current_time = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "# Access the model name from the GenerativeModel object\n",
        "\n",
        "eval_entry_name = f\"model_{model}_temp_{temperature_val}_time_{current_time}\"\n",
        "\n",
        "eval_results_to_compare.append({\n",
        "    \"name\": eval_entry_name,\n",
        "    \"eval_result\": eval_result\n",
        "})\n",
        "\n",
        "print(f\"Added evaluation result: {eval_entry_name}\")\n",
        "display(eval_result.summary_metrics)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 108
        },
        "id": "D0-qjzM6v1Kq",
        "outputId": "79261a47-5e0b-4f73-e804-eb23b38ebe91"
      },
      "id": "D0-qjzM6v1Kq",
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Added evaluation result: model_<vertexai.generative_models.GenerativeModel object at 0x7ac9206ca960>_temp_0_time_20251203-201030\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "{'row_count': 3,\n",
              " 'instruction_following/mean': np.float64(5.0),\n",
              " 'instruction_following/std': 0.0}"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Compare results\n",
        "\n",
        "because I define 'eval_results_to_compare' up at the top You can back track, update the model or the 'instructions' for function2, 'foo2', and can see the comparison here."
      ],
      "metadata": {
        "id": "N_sqNIcy0m2N"
      },
      "id": "N_sqNIcy0m2N"
    },
    {
      "cell_type": "code",
      "source": [
        "for item_dict in eval_results_to_compare:\n",
        "  print(\"=========\")\n",
        "  print(f\"Evaluation Name: {item_dict['name']}\")\n",
        "  # Assuming 'fluency/score' is the metric of interest from previous outputs\n",
        "  for item in item_dict['eval_result'].summary_metrics:\n",
        "    display(item + \":  \" + str(item_dict['eval_result'].summary_metrics[item]))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 309
        },
        "id": "-SjDwiJgxAmO",
        "outputId": "ea3f1c29-02dc-4522-a67a-7b247c9cae69"
      },
      "id": "-SjDwiJgxAmO",
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=========\n",
            "Evaluation Name: model_<vertexai.generative_models.GenerativeModel object at 0x7ac9206ca960>_temp_2_time_20251203-200809\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "'row_count:  3'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "'instruction_following/mean:  5.0'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "'instruction_following/std:  0.0'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=========\n",
            "Evaluation Name: model_<vertexai.generative_models.GenerativeModel object at 0x7ac9206ca960>_temp_2_time_20251203-200856\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "'row_count:  3'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "'instruction_following/mean:  5.0'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "'instruction_following/std:  0.0'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=========\n",
            "Evaluation Name: model_<vertexai.generative_models.GenerativeModel object at 0x7ac9206ca960>_temp_0_time_20251203-201030\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "'row_count:  3'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "'instruction_following/mean:  5.0'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "'instruction_following/std:  0.0'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zBhm95ePyWuY"
      },
      "id": "zBhm95ePyWuY",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    },
    "colab": {
      "provenance": [],
      "name": "lab3-evaluation"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}